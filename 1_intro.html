<div id="warm-up." class="slide section level1">
<h1>Warm Up.</h1>
<!-- - How computers know how to do things?
- Two ways:
    1. programming: steps detailed by human programmer
    2. learning: without being specifically told 
- Example 1: machine translation
    1. programming: writing many rules to replace and reposition words, e.g., 
    _Do you speak Julia?_ _Sprechen Sie Julia?_   
    2. learning: feeding the computer many bilingual documents 
- Example 2: sorting 
    1. programming: Quicksort, etc. 
    2. learning: feeding the computer many pairs of unsorted and sorted list of numbers. 
- The first approach in the context of AI is also called rule-based system or expert system, e.g. MyCin, Grammarly. -->
<ul class="incremental">
<li>Every object persists in its state of rest or uniform motion in a straight line unless it is compelled to change that state by forces impressed on it</li>
<li><span class="math inline"><em>e</em> = <em>m</em><em>c</em><sup>2</sup></span></li>
<li><span class="math inline"><em>O</em>(<em>n</em>log (<em>n</em>))</span></li>
</ul>
</div>
<div id="why-ml-is-attractive" class="slide section level1">
<h1>Why ML is attractive</h1>
<!-- - We are lazy. We want to shift the heavy lifting to the computers. 
- We are incompetent. No kidding! Sometimes it is very difficult to come up with step-by-step instructions. 
- Examples: Self-driving, AlphaGo, Automated circuit routing, Machine translation, Commonsense reasoning, text entailment, Document generation, auto-reply of messages/emails, [fly a helicoper inversely](https://www.youtube.com/watch?v=M-QUkgk3HyE), [van-Gogh-lize paints](https://blogs.nvidia.com/blog/2016/05/25/deep-learning-paints-videos/). 
- It is a dream. "Creating an artificial being has been the dream since the beginning of science." -- Movie A.I., Spielberg et al., 2001 -->
<ul class="incremental">
<li>Supervised</li>
<li>Unsupervised</li>
<li>Reinforcement</li>
</ul>
</div>
<div id="section" class="slide section level1">
<h1></h1>
<!-- ML (in current approaches) is about finding/approximating functions. 

- Supervised, finding $\hat{f}(x) \approx f(x)$ with ground truth provided by human. 
    * Let $x$ and $y$ be two (vectors of) variables, and a function connecting them $y = f(x)$ But only god knows $f$. 
    * We construct another function $\hat{f}$ to approximate $f$ such that $\hat{y} = \hat{f}(x) \approx y = f(x)$ for a(ny) given $x$. 
    * **Supervised** because we  provide many pairs of $x$'s and $y$'s for the computer to know the difference between $\hat{y}$ and $y$ on a large pool of samples. 
    * Examples: object detection from images, [Flavia](http://flavia.sourceforge.net/), [CPU branch prediction](https://www.electronicdesign.com/technologies/microprocessors/article/21802106/ai-helps-amds-ryzen-take-on-intel),  [COVID-19 diagnosis from blood profile](https://arxiv.org/abs/2005.06546), [Epileptic EEG recognition](https://www.technologyreview.com/2009/04/29/213440/a-neural-net-that-diagnoses-epilepsy/), [depression treatment from brain shapes](https://mfr.osf.io/render?url=https://osf.io/b58jr/?action=download%26mode=render).
    * Beyond categorization/classification: [Mflux](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004838), [Review helpfulness prediction](https://www.aclweb.org/anthology/P15-2007.pdf), [Document summarization](https://www.aclweb.org/anthology/E17-2112.pdf), predict house prices
- Unsupervised, finding $\hat{f}(x)$ without ground truth
- Reinforcement, let the machine find ground truth itself -->
<p>What is the minimal dimension of the vector? 5.</p>
</div>
<div id="representation-of-x" class="slide section level1">
<h1>Representation of <span class="math inline"><em>x</em></span></h1>
<ul class="incremental">
<li><span class="math inline"><em>x</em></span> is usually not a simple (vector of) number(s). How to tell it to a computer?</li>
<li>Example: bananas vs. apples</li>
<li><strong>Feature engineering</strong>: manually craft functions to <strong>extract</strong> features from raw data, e.g,. SIFT, bag-of-words.</li>
<li>Automated feature extraction in deep learing: E.g., filters in CNNs.</li>
<li>If <span class="math inline"><em>x</em></span> involves categorical values (e.g., gender), there are usually two approaches: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"><strong>One-hot encoding</strong></a> and <a href=""><strong>embedding</strong></a> (in DL context, to be discussed later).</li>
</ul>
</div>
<div id="supervised-ml" class="slide section level1">
<h1>Supervised ML</h1>
<ul class="incremental">
<li>Given many pairs of inputs and outputs: <span class="math inline">{(<strong>X</strong><sub><strong>1</strong></sub><strong>,</strong> <strong>y</strong><sub><strong>1</strong></sub>), (<strong>X</strong><sub><strong>2</strong></sub><strong>,</strong> <strong>y</strong><sub><strong>2</strong></sub>), …, (<strong>X</strong><sub><strong>N</strong></sub><strong>,</strong> <strong>y</strong><sub><strong>N</strong></sub>)}</span>,</li>
<li>that underline a “black-box” function <span class="math inline"><em>f</em> : ℝ<sup><em>n</em></sup> ↦ ℝ<sup><em>m</em></sup></span> such that <span class="math inline">∀<em>i</em> ∈ [1..<em>n</em>], <em>f</em>(<strong>X</strong><sub><em>i</em></sub>) = <strong>y</strong><sub><em>i</em></sub></span>,</li>
<li>construct a function <span class="math inline"><em>f̂</em></span> that approximates the function <span class="math inline"><em>f</em></span>.</li>
<li>“approximate”: usually <span class="math inline">min ||<em>f̂</em>(<em>x</em>) − <em>f</em>(<em>x</em>)||<sup><em>p</em></sup></span> where <span class="math inline"><em>p</em></span> is usually 1 or 2. <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">See <span class="math inline">ℓ<sub><em>p</em></sub></span>-norm</a> . <!-- - In other words, $f$ is a black box. And we need to find $\hat{f}$ that mimick the black box.  --></li>
<li>The process of finding the approximation function <span class="math inline"><em>f̂</em></span> is called <strong>training</strong> or <strong>learning</strong>.</li>
<li><span class="math inline"><em>f̂</em></span> is called a <strong>model</strong> or an <strong>estimator</strong>.</li>
<li><span class="math inline"><strong>X</strong><sub><strong>i</strong></sub></span>: an <strong>input</strong> (especially when raw data is used as the input) or <strong>feature vector</strong> (if using feature engineering).</li>
<li><span class="math inline"><strong>y</strong><sub><strong>i</strong></sub></span>, often <span class="math inline"> ∈ ℝ<sup>1</sup></span> a <strong>label</strong> (in classification) or <strong>target</strong> (used more generally and lately).</li>
<li>Classification vs. Regression: When <span class="math inline"><em>y</em></span> is continuous or discrete. In modern DL context, such division is usually no mentioned, expecially in generative tasks.</li>
</ul>
</div>
